<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>CTC (Connectionist Temporal Classification)</title>
    <link href="/2021/10/20/CTC/"/>
    <url>/2021/10/20/CTC/</url>
    
    <content type="html"><![CDATA[<h3 id="CTC"><a href="#CTC" class="headerlink" title="CTC"></a>CTC</h3><p>论文地址：<a href="http://people.idsia.ch/~santiago/papers/icml2006.pdf">http://people.idsia.ch/~santiago/papers/icml2006.pdf</a></p><h4 id="1-背景"><a href="#1-背景" class="headerlink" title="1.背景"></a>1.背景</h4><p>​       CTC本质上就是一个序列学习任务中使用到的encoder-decoder网络结构：</p><p><img src="/2021/10/20/CTC/12.jpg" style="zoom:40%;"></p><p>​        在序列学习任务中，模型对训练样本一般有这样的依赖条件：输入序列和输出序列之间的映射关系已经事先标注好了。比如，在词性标注任务中，训练样本中每个词（或短语）对应的词性会事先标注好，如下图（DT、NN等都是词性的标注<a href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html">链接</a>）。由于输入序列和输出序列是一一对应的，所以模型的训练和预测都是端到端的，即可以根据输出序列和标注样本间的差异来直接定义模型的Loss函数，传统的RNN训练和预测方式可直接适用。</p><p><img src="/2021/10/20/CTC/1.jpg" style="zoom:40%;"></p><p>​        然而，在语音识别、手写字识别等任务中，由于音频数据和图像数据都是从现实世界中将模拟信号转为数字信号采集得到，这些数据天然就很难进行“分割”，这使得我们很难获取到包含输入序列和输出序列映射关系的大规模训练样本（人工标注成本巨高）。因此，在这种条件下，RNN无法直接进行端到端的训练和预测。</p><p>​        如下图，输入是“apple”对应的一段说话音频和手写字图片，从连续的音频信号和图像信号中逐一分割并标注出对应的输出序列非常费时费力，在大规模训练下这种数据要求是完全不切实际的。而如果输入序列和输出序列之间映射关系没有提前标注好，那传统的RNN训练方式就不能直接适用了，无法直接对音频数据和图像数据进行训练。</p><p><img src="/2021/10/20/CTC/3.jpg" style="zoom:40%;"></p><p><img src="/2021/10/20/CTC/2.jpg" style="zoom:40%;"></p><p>对音频“分割”并标注映射关系的数据依赖是不切实际的，实际情况是对音频按照时间窗口滑动来提取特征，例如按照每帧音频提取特征：</p><p><img src="/2021/10/20/CTC/6.jpg" style="zoom:50%;"></p><p>因此，在语音识别、手写数字识别等领域中，由于数据天然无法切割，且难以标注出输入和输出的序列映射关系，导致传统训练方法不能直接适用。那么，如何让RNN模型实现端到端的训练成为了关键问题。</p><p>Connectionist Temporal Classification（CTC）是Alex Graves等人在ICML 2006上提出的一种端到端的RNN训练方法，它可以让RNN直接对序列数据进行学习，而无需事先标注好训练数据中输入序列和输入序列的映射关系。</p><h4 id="2-CTC算法"><a href="#2-CTC算法" class="headerlink" title="2.CTC算法"></a>2.CTC算法</h4><p>给定输入序列$X=[x_1,x_2,…,x_T]$ 以及对应的标签数据$Y=[y_1,y_2,…,y_U]$  ,如语音识别中的音频文件和文本文件。我们的目的是找到 $X$ 到 $Y$ 的一个映射函数</p><p>对比传统的分类方法，有如下难点：</p><ol><li>$X$ 和 $Y$ 的长度都是变化的且不相等的；</li><li>对于一个端到端的模型，我们并不希望手动设计$X$ 和 $Y$ 的之间的对齐。</li></ol><h5 id="2-1-对齐方式"><a href="#2-1-对齐方式" class="headerlink" title="2.1 对齐方式"></a>2.1 对齐方式</h5><p>CTC本身是不需要对齐的，但是我们需要知道 $X$ 的输出路径和最终输出结果的对应关系，因为在CTC中，多个输出路径可能对应一个输出结果。例如在语音识别的任务中，输入 $X$ 是“CAT”的语音，输出 $Y$ 是文本[C, A, T]。将 $X$ 分割成若干个语音帧，每一帧得到一个输出，一个最简单的解决方案是合并连续重复出现的字母：</p><p><img src="/2021/10/20/CTC/4.jpg" style="zoom:50%;"></p><p>直接这样对齐有两个缺点：</p><ol><li>几乎不可能将 $X$的每个时间片都和输出Y对应上，例如OCR中字符的间隔，语音识别中的停顿;</li><li>不能处理有连续重复字符出现的情况，例如单词“HELLO”，按照上面的算法，输出的是“HELO”而非“HELLO”。</li></ol><p>为了解决上面的问题，CTC引入了空白字符 $\epsilon$ ，例如OCR中的字符间距，语音识别中的停顿均表示为 $\epsilon$ 。所以，CTC的对齐涉及去除重复字母和去除 $\epsilon$ 两部分：</p><p><img src="/2021/10/20/CTC/5.jpg" style="zoom:50%;"></p><h5 id="2-2-损失函数"><a href="#2-2-损失函数" class="headerlink" title="2.2  损失函数"></a>2.2  损失函数</h5><p>对于给定的训练数据$(X,Y)$ ,本质上我们需要对$P(Y|X)$建模，但根据以上定义的对齐方式，多个输出路径均可能对应同一个输出结果$Y$</p><p><img src="/2021/10/20/CTC/7.jpg" style="zoom:80%;"></p><p>也就是说，对于标签 $Y$ ，其关于输入 $X$ 的后验概率可以表示为所有映射为  的$Y$路径之和，我们的目标就是最大化 $Y$ 关于 $X$ 的后验概率  。假设$P(Y|X)$每个时间片的输出是相互独立的，则路径的后验概率是每个时间片概率的累积</p><script type="math/tex; mode=display">P(Y|X)=\sum_{o\in O} P(o|X) = \sum_{o\in O} \prod_{t=1}^T p_t(o_t|x_t)</script><p>其中，$(X,Y)$ 分别表示输入及输出，$O$ 表示所有可能的对齐方式，$p_t(o_t|x_t)$ 表示在时刻t输出字符 $o_t$的概率.</p><p>因此，只需要穷举出所有的$O$ ，累加一起即可得到$P(Y|X)$，从而使得RNN模型对最终的label进行建模。</p><p>对于一个训练集合$S{(X,Y)}$ ，CTC的损失函数可定义为训练集S所有样本的负对数似然概率之和：</p><script type="math/tex; mode=display">L(S) = -\sum_{(X,Y)\in S} ln(P(Y|X))</script><p><img src="/2021/10/20/CTC/13.jpg" style="zoom:80%;"></p><p>但这样的CTC算法存在性能问题，对于一个时间片长度为 $T$ 的 $N$ 分类任务，所有可能的路径数为 $T^N$ ，在很多情况下，用于计算Loss几乎是不现实的。在CTC中参照了HMM中的算法，采用了动态规划的思想来进行计算.</p><h5 id="2-3-前后向概率及训练"><a href="#2-3-前后向概率及训练" class="headerlink" title="2.3 前后向概率及训练"></a>2.3 前后向概率及训练</h5><p>为了更形象表示问题的搜索空间，将所有的路径放在坐标轴中，并把输出序列$Y=[y_1,y_2,…,y_U]$做标准化处理，输出序列中间和头尾都加上$\epsilon$，得到$Z=[\epsilon, y_1,\epsilon,y_2,…,y_U,\epsilon]$, 横轴的单位是$X$对应的时间步， 纵轴是标签$Y$插入$\epsilon$后的序列$Z$，例如：</p><script type="math/tex; mode=display">Y=ZOO \Rightarrow Z=\epsilon Z \epsilon O \epsilon O \epsilon</script><p>以“ZOO”这个标签为例，按照时间步（T=9）展开，则可将$P(Y|X)$ 的所有可能的合法路径表示如下图：</p><p><img src="/2021/10/20/CTC/8.jpg" style="zoom:80%;"></p><p>为了让所有的路径都能在图中有唯一且合法的表示，节点转换有一定约束。</p><p>接下来利用动态规划思想计算所有路径的概率总和：</p><p>定义在$t$时刻经过节点$s$的全部前缀子路径的概率总和为前向概率为 $\alpha_t(s)$，表示已经输出部分观察值$x_1,x_2,…,x_t$ 且$t$时刻输出标签为$s$ 的概率，用$p_t(z_s|x_t)$ 表示在$t$时刻输出字符$z_s$ 的概率</p><p><strong>Case 1</strong>:   </p><p>如果 $z_s=\epsilon$, 则 $\alpha_t(s)$ 只能由 $\alpha_{t-1}(s_{t-1})$ 或者 $\alpha_{t-1}(s)$ 得到，如果$z_s\ne\epsilon$ 但$z_s$为连续字符，即$z_s=z_{s-2}$，则 $\alpha_t(s)$ 也只能由 $\alpha_{t-1}(s-1)$ 或者 $\alpha_{t-1}(s)$ 得到，即：</p><script type="math/tex; mode=display">\alpha_t(s) = (\alpha_{t-1}(s) +\alpha_{t-1}(s-1))*p_t(z_s|x_t)</script><p><img src="/2021/10/20/CTC/8_1.png" style="zoom:80%;"></p><p><strong>Case 2</strong>:   </p><p>如果$z_s\ne\epsilon$ 且不为连续字符，则$\alpha_t(s)$ 可以由 $\alpha_{t-1}(s_{t-2})$ ， $\alpha_{t-1}(s_{t-1})$ 以及 $\alpha_{t-1}(s)$ 得到，即：</p><script type="math/tex; mode=display">\alpha_t(s) = (\alpha_{t-1}(s) +\alpha_{t-1}(s-1)+\alpha_{t-1}(s-2))*p_t(z_s|x_t)</script><p><img src="/2021/10/20/CTC/8_2.png" style="zoom:80%;"></p><p>可得到CTC的前向概率递推计算：</p><script type="math/tex; mode=display">初始化 =\begin{cases}\alpha_1(1)=P(\epsilon|x_1)  \\\alpha_1(2)=P(z_1|x_1) \\\alpha_1(s)=0, s\gt2\end{cases}</script><script type="math/tex; mode=display">迭代计算  \alpha_t(s) = \begin{cases}(\alpha_{t-1}(s) +\alpha_{t-1}(s-1))*p_t(z_s|x_t),  如果z_s=\epsilon 或z_s=z_{s-2} \\\alpha_t(s) = (\alpha_{t-1}(s) +\alpha_{t-1}(s-1)+\alpha_{t-1}(s-2))*p_t(z_s|x_t), & 其他\end{cases}</script><p>同理，定义后向概率$\beta_t(s)$ 表示在$t$时刻输出标签为$s$ 及输出观察值序列为$x_t,x_{t+1},…,x_T$ 的概率，同样可得 CTC的后向概率计算：</p><script type="math/tex; mode=display">初始化 =\begin{cases}\beta_T(2U+1)=P(z_{2U+1}|x_t)  \\\beta_T(2U)=P(Z_{2U}|x_t) \\\beta_T(s)=0, s\lt2U\end{cases}\\</script><script type="math/tex; mode=display">迭代计算 \beta_t(s) = \begin{cases}(\beta_{t+1}(s) +\beta_{t+1}(s+1))*p_t(z_s|x_t),  如果z_s=\epsilon 或z_s=z_{s-2} \\(\beta_{t+1}(s) +\beta_{t+1}(s+1)+\beta_{t+1}(s+2))*p_t(z_s|x_t), & 其他\end{cases}</script><p>利用前向概率和后向概率计算CTC的损失函数：</p><script type="math/tex; mode=display">P(Y|X)=\sum_{s=1}^{2U+1} \frac {\alpha_t(s)\beta_t(s)}{P(z_s|x_t)} \\</script><script type="math/tex; mode=display">L(S) = -\sum_{(X,Y)\in S} ln(P(Y|X)) =-\sum_{(X,Y)\in S}ln(\sum_{s=1}^{2U+1} \frac {\alpha_t(s)\beta_t(s)}{P(z_s|x_t)})</script><p>由此，即可用BPTT算法对CTC进行训练：</p><p><img src="/2021/10/20/CTC/11.jpg" style="zoom:80%;"></p><h4 id="3-解码"><a href="#3-解码" class="headerlink" title="3. 解码"></a>3. 解码</h4><p>给定一个输入序列 $X$ ，我们需要找到最可能的输出:</p><script type="math/tex; mode=display">Y^*=arg\max_YP(Y|X)</script><p><strong>Greedy Search</strong>: 每个时间片均取该时间片概率最高的节点作为输出</p><p><img src="/2021/10/20/CTC/9.jpg" style="zoom:60%;"></p><p><strong>Beam search</strong></p><p><img src="/2021/10/20/CTC/10.jpg" style="zoom:60%;"></p><p><strong>Prefix Beam Search</strong></p><p>有许多不同的路径在many-to-one map的过程中是相同的，但beam search却会将一部分舍去，这导致了很多有用的信息被舍弃了，基本的思想是在搜索过程中不断的合并相同的前缀：</p><p><img src="/2021/10/20/CTC/14.jpg" style="zoom:60%;"></p>]]></content>
    
    
    <categories>
      
      <category>语音识别</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>FlowScope:Spotting Money Laundering Based on Graphs</title>
    <link href="/2021/09/16/FlowScope%20Spotting%20Money%20LaunderingBased%20on%20Graphs/"/>
    <url>/2021/09/16/FlowScope%20Spotting%20Money%20LaunderingBased%20on%20Graphs/</url>
    
    <content type="html"><![CDATA[<h1 id="一、总览"><a href="#一、总览" class="headerlink" title="一、总览"></a>一、总览</h1><p>顾名思义，原文旨在通过图相关方法，发现洗钱行为。洗钱行为表示犯罪分子通过银行将大量的非法所得转移至无法追溯的终点，而不被发现。现有的关于反欺诈的图相关方法主要关注紧密子图的发现，而不考虑洗钱过程中存在大量金钱转移的事实，从而降低了探查准确率。原文提出的FlowScope 方法不仅可以量化整个洗钱过程，而且从理论层面可以提出了正常交易中不被探查的金额上限。</p><p>原文地址：<a href="http://www.shichuan.org/doc/78.pdf">http://www.shichuan.org/doc/78.pdf</a></p><p>源码地址：<a href="https://github.com/aplaceof/FlowScope">https://github.com/aplaceof/FlowScope</a></p><h1 id="二、简介与相关工作"><a href="#二、简介与相关工作" class="headerlink" title="二、简介与相关工作"></a>二、简介与相关工作</h1><p>在已有的很多反洗钱算法中，均有不同程度的缺陷，其中比较典型的问题包括：</p><ol><li>忽略金钱转移链的模式，同时忽略交易过程中的复杂依赖，特别是节点间的复杂资金往来记录，从而会降低准确率。</li><li>有些考虑金钱转移链的模式的，需要用监督学习的方案进行学习，而反洗钱场景中，有效标签量非常难以获取。同时，在很多通过对抗网络获得的模型，因为样本的严重不平衡，可能存在模型健壮性欠佳的情况。</li><li>部分以业务经验为基础的算法，虽然短期有效，但容易被违法者攻破。</li></ol><p>而FlowScope主要包含以下优势：</p><ol><li>提出一种新的洗钱发现方法，这种方法可以很好的发现紧密交易和多步骤洗钱的信息。</li><li>严谨的理论证明，不仅证明此方法的有效性，同时可以给出犯罪分子在该模型下可以转出资金而不被发现的上限。</li><li>高效性和健壮性。</li><li>可扩展性，同时在上面的github地址给出扩展样例。</li></ol><h1 id="三、问题定义"><a href="#三、问题定义" class="headerlink" title="三、问题定义"></a>三、问题定义</h1><p>在定义问题前，我们阐述洗钱过程中的两个重要特点（从我个人工作经历来看，对下面两点高度赞同）：</p><ol><li>密集的转账记录：参与洗钱卡，会产生大量的转入转出记录，并且会总的转入转出金额很大。</li><li>中间账户的余额通常很小：中间账户一般作为桥梁，不会存放资金。（个人经验：在个别情况下，还存在转入后快速转出，中间停留时间在数分钟到数小时不等。）</li></ol><p>此方法重点关注这类型的账号。</p><p>核心问题可以定义为：</p><p>在一个资金流水图中$\mathcal{G} = (\mathcal{V}, \mathcal{E})$，其中卡号作为节点$\mathcal{V}$，转移资金量作为边$\mathcal{E}$。那么目的可以是找到一个$\mathcal{G}$的子图，并满足以下条件：</p><ol><li>交易流水在转入这批中间账户和转出这批中间账户的过程中有很大量。</li><li>由下面的定义的新标准，保证资金流动量在子图中最大。那么定义的ML metric则即为重要。</li></ol><p>下图展示在后续解释中可能出现的重要符号：</p><p><img src="G:\work\模型中心\19. X_lab\自己读的内容\17_FLOWSCOPE\11111.jpg" alt="11111"></p><p>通常，图$\mathcal{G} = (\mathcal{V}, \mathcal{E})​$中的节点$\mathcal{V} = \mathcal{X} \bigcup \mathcal{W} \bigcup \mathcal{Y}​$，其中$\mathcal{W}​$表示洗钱桥梁，$\mathcal{X}​$和$\mathcal{Y}​$分别代表金钱来源（整体情况为转入中间层）和资金去向（整体情况为从中间层转出）的卡号。一条边$(i,j)\in \mathcal{E}​$表示在节点集合$\mathcal{V}中，​$卡号$v_i​$转至卡号$v_j​$的资金量$e_{ij}​$。</p><h1 id="四、核心"><a href="#四、核心" class="headerlink" title="四、核心"></a>四、核心</h1><p>这部分主要分三个小点，阅读建议着重理解ML metric。只有足够好的理解第一部分，才能更好的理解第二部分。</p><h2 id="1-ML-metric"><a href="#1-ML-metric" class="headerlink" title="1. ML metric"></a>1. ML metric</h2><p>定义ML metric的目的是发现紧密资金转移的现象。</p><p>定义节点子集$\mathcal{S} = \mathcal{A} \bigcup \mathcal{M_1} \bigcup … \bigcup \mathcal{M_{k-2} \bigcup \mathcal{C}}$，其中$\mathcal{A} \subseteq \mathcal{X}, \mathcal{M_i} \subseteq \mathcal{W}, \mathcal{C} \subseteq \mathcal{Y}$，其中在$\mathcal{M_1}, … , \mathcal{M_{k-2}}$中可能存在节点重合的情况。但在实际情况中，如果资金链条形成闭环更容易被执法者发现，因此我们主动忽略这样的影响。在上面的式子中，我们可以认为总共有$k$层数据，其中第0层为$\mathcal{X}$，第$k$层为$\mathcal{Y}$。</p><p>同时定义$e_{ij}​$表示为从节点$v_i​$转至节点$v_j​$的资金总量。那么对于节点$v_i \in \mathcal{M_l}, l\in {1, 2, … , k-2}​$，定义在子集$\mathcal{S}​$总的资金转出量与资金转入量分别为：</p><script type="math/tex; mode=display">d^+_i(s) = \sum_{v_j\in M_{l+1}\wedge(i,j)\in \mathcal{E}} e_{ij}\\d^-_i(s) = \sum_{v_k\in M_{l-1}\wedge(k,i)\in \mathcal{E}} e_{ki}</script><p>下面定义在子集$\mathcal{S}$下资金流转的最小，最大量分别为：</p><script type="math/tex; mode=display">f_i(s) = \min\{d^+_i(S), d^-_i(S)\}, \forall v_i \in \mathcal{M_l}\\q_i(s) = \max\{d^+_i(S), d^-_i(S)\}, \forall v_i \in \mathcal{M_l}</script><p>那么，则可以定义ML metric为：</p><script type="math/tex; mode=display">g^k(\mathcal{S}) = \frac{1}{|\mathcal{S}|}\sum^{k-2}_{l=1}\sum_{v_i\in \mathcal{M_l}} f_i(\mathcal{S}) - \lambda (q_i(\mathcal{S}) - f_i(\mathcal{S}))\\= \frac{1}{|\mathcal{S}|}\sum^{k-2}_{l=1}\sum_{v_i\in \mathcal{M_l}} (1+\lambda)f_i(\mathcal{S}) - \lambda q_i(\mathcal{S}), k\geq 3</script><p>有定义可知$q_i(\mathcal{S}) - f_i(\mathcal{S})$是剩余在$v_i$卡号上的余额，这个可以被认为是针对ML过程中桥梁卡号通常余额很小的惩罚项。对上式中的$\lambda$可以定义为转账过程中必须产生的手续费等开销。这样我们定义的$g(\mathcal{S})$则可以认为是洗钱后的剩余利润。</p><p><img src="G:\work\模型中心\19. X_lab\自己读的内容\17_FLOWSCOPE\2222.jpg" alt="2222"></p><p>举个例子，上图中表示的是一个ML过程。A（资金流入卡号），M（桥梁卡号），C（资金流出卡号）分别有4,12,2个卡号。$v_5$转移了将近452.1M元，同时$q_5(\mathcal{S}) - f_5(\mathcal{S}) \approx 0$，这就表示此卡有大量的流水转入转出，并且卡账剩余金额很少。ML metric 可以很好的捕捉到这些特点。</p><h2 id="2-FlowScope"><a href="#2-FlowScope" class="headerlink" title="2. FlowScope"></a>2. FlowScope</h2><p>FlowScope的提出，目的在于找到一个$\mathcal{S}$使得我们的目标$g(\mathcal{S})$最大化。先制作一个优先树，那么对于任意节点$v_i$存在的权重定义如下：</p><script type="math/tex; mode=display">w_i(\mathcal{S}) = \{\begin{array}{cols}                     f_i(\mathcal{S}) - \frac{\lambda}{1+\lambda}q_i(\mathcal{S}),  & \mbox{if} & v_i \in \mathcal{M_l}                    \\                    d_i(\mathcal{S}), & \mbox{if} & v_i \in \mathcal{A} \cup \mathcal{C}                    \end{array}</script><p>那么下面则可以给出相应的贪婪优化方法：</p><ol><li>确定图。</li><li>从现有图中移除有最小权重的点（为了最大化我们的目标）。</li><li>更新图中受影响节点的权重，并计算当前图下的目标$g(\mathcal{S})$。</li><li>重复2-3过程，直至A，M，C中任意一个集合的节点全部被移除。</li><li>得到最大的$g(\hat{\mathcal{S}})$，同时可以找到对应子图$\hat{\mathcal{S}}$。</li></ol><p><img src="G:\work\模型中心\19. X_lab\自己读的内容\17_FLOWSCOPE\33333.jpg" alt="33333"></p><p>截止此处，原文中最核心的部分已经介绍完成。我个人对以下两个部分有一点疑问：</p><ol><li>这里有一个比较讨厌的部分是对桥梁节点的“层级”进行定义或者先验已知。这部分内容很讨厌的原因是，我们不仅不太好区分到底作为桥梁的卡有多少个“层级”，同时难以区分到底哪张卡应该隶属于哪个层级也尝尝在实际情况中遇到困难。</li><li>在贪婪算法部分，停止条件是当一个节点集合为空则停止搜索。这部分内容从直觉上说可能会有点问题。在实现的时候会做一些测试。</li></ol><h2 id="3-分析部分"><a href="#3-分析部分" class="headerlink" title="3. 分析部分"></a>3. 分析部分</h2><h3 id="3-1-Theoretical-Bound"><a href="#3-1-Theoretical-Bound" class="headerlink" title="3.1 Theoretical Bound"></a>3.1 Theoretical Bound</h3><p>给定图和metric ML定义。那么对于任意子图，存在：</p><script type="math/tex; mode=display">g(\hat{\mathcal{S}}) \geq \frac{|\mathcal{M^\prime}|}{|\mathcal{S^\prime}|}(g(\mathcal{S}^\star) - \lambda \epsilon)</script><p>其中$\epsilon = \max_{v_i \in \mathcal{S}^\star} \{q_i(\mathcal{V}) - q_i(\mathcal{S^\star})\}​$，表示为在该子图内转移资金量最多的账户接收转出到其他非洗钱账户的金额。</p><h3 id="3-2-Bounding-Money-Laundering"><a href="#3-2-Bounding-Money-Laundering" class="headerlink" title="3.2  Bounding Money Laundering"></a>3.2  Bounding Money Laundering</h3><p>给定子图，该子图每个账号进行洗钱行为但不被发现的资金量上限为：</p><script type="math/tex; mode=display">\frac{\sum_{v_i \in \mathcal{S}^\star}f_i(\mathcal{S}^\star)}{n_0} \leq \frac{1}{1-\lambda \eta}(\frac{|\mathcal{S}^\prime|}{|\mathcal{M}^\prime|}g(\hat{\mathcal{S}}) + \lambda \epsilon)</script><p>其中新出现的符号中，$n_0​$表示该子图的卡号数量，$\eta​$表示可能存在的其他费用。其定义为：</p><script type="math/tex; mode=display">\eta = \frac{\sum_{v_i \in \mathcal{S}^\star}(q_i(\mathcal{S}^\star) - f_i(\mathcal{S}^\star))}{\sum_{v_i \in \mathcal{S}^\star}f_i(\mathcal{S}^\star)}, \eta \in [0, \frac{1}{\lambda}]</script><p>这两个部分的内容可以作为排查过程中的其他评价指标进行输出。</p><h1 id="五、实验"><a href="#五、实验" class="headerlink" title="五、实验"></a>五、实验</h1><p>实验数据：</p><ol><li>CBANK，匿名银行真实数据。</li><li>CFD（Czech Financial Dataset），另一个匿名银行数据，曾经用于比赛。</li></ol><p>对比方法SpokEN，Fraudar，D-Cube，HoloScope，RRCF。评价方法选择FAUC与F1两个评价指标。并主要点评工作效率，模型效果，健壮性。测试效果见图。</p><p><img src="G:\work\模型中心\19. X_lab\自己读的内容\17_FLOWSCOPE\4444.jpg" alt="4444"></p><p>核心结论如下：</p><ol><li>相比其他方法，此方法能够更快、更准确的发现洗钱行为。模型计算时间随图内边数量增加称线性关系。</li><li>在面对更长的资金转移链条时，表现的更稳定更健壮。</li><li>模型对相关先验参数不敏感，如：$\lambda$。</li></ol><h1 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h1><p>原文提出了一种评价洗钱行文的量化指标，并且求解思路清晰，可执行性强。建议阅读内容如下：</p><blockquote><p>Guha,S.;Mishra,N.;Roy,G.;andSchrijvers,O. 2016. Robustrandomconferenceutforestbasedanomalydetectionon streams. In International conference on machine learning, 2712–2721.</p><p>Hooi, B.; Song, H. A.; Beutel, A.; Shah, N.; Shin, K.; and Faloutsos, C. 2016. Fraudar: Bounding graph fraud in the face of camouﬂage. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 895–904. ACM.</p><p>Liu, S.; Hooi, B.; and Faloutsos, C. 2017. Holoscope: Topology-and-spike aware fraud detection. In Proceedings<br>ofthe2017ACMonConferenceonInformationandKnowledge Management, 1539–1548. ACM.</p><p>Liu, S.; Hooi, B.; and Faloutsos, C. 2018. A contrast metric for fraud detection in rich graphs. IEEE Transactions on Knowledge and Data Engineering.</p><p>Prakash, B. A.; Sridharan, A.; Seshadri, M.; Machiraju, S.; and Faloutsos, C. 2010. Eigenspokes: Surprising patterns andscalablecommunitychippinginlargegraphs. InPaciﬁcAsiaConferenceonKnowledgeDiscoveryandDataMining, 435–448. Springer.</p><p>Shin, K.; Hooi, B.; Kim, J.; and Faloutsos, C. 2017. Dcube: Dense-block detection in terabyte-scale tensors. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, 681–689. ACM.</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Graph Embedding</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Scalable out-of-sample extension of graph embeddings using deep neural networks</title>
    <link href="/2021/09/16/Scalable%20out-of-sample%20extension%20of%20graph%20embeddings%20using%20deep%20neural%20networks/"/>
    <url>/2021/09/16/Scalable%20out-of-sample%20extension%20of%20graph%20embeddings%20using%20deep%20neural%20networks/</url>
    
    <content type="html"><![CDATA[<h3 id="Scalable-out-of-sample-extension-of-graph-embeddings-using-deep-neural-networks"><a href="#Scalable-out-of-sample-extension-of-graph-embeddings-using-deep-neural-networks" class="headerlink" title="Scalable out-of-sample extension of graph embeddings using deep neural networks"></a>Scalable out-of-sample extension of graph embeddings using deep neural networks</h3><ol><li><p>背景</p><p>对于一系列具有高维特征的样本集$X$（如图片）,如果要进行分类或聚类等任务，一个重要的处理方法是先进行数据降维，再使用分类器或聚类模型，提高精度和效率。</p><p>谱分解问题：令$X=\{x_1, x_2,\dots,x_n \}$ 是需要进行嵌入的特征样本，其中$x_i \in R^d.$ 注意这里的d不是代表编码向量的长度，而是人工提取的特征长度。利用某个核函数$K:R^d \times R^d \rightarrow R$ ,可以定义样本X的相似矩阵A, 其中每个元素为$A_{ij} = K(x_i, x_j),$ 则A的拉普拉斯矩阵和度矩阵分别为：</p></li></ol><script type="math/tex; mode=display">\mathcal L = I - D^{-1/2}A D^{-1/2},\quad  D = diag(\{\sum_{j=1}^n A_{ij}, i=1,2,\dots,n \})</script><p>​        于是每个特征样本的编码转换为求解$\mathcal L$ 的特征值和特征向量的问题，即谱分解$\mathcal L = U \sum U^T$, 其中$\sum$是对角矩阵，且元素为拉普拉斯矩阵的奇异值，且按从高到低排列。取U的前$d’$列得到子矩阵$U_{d’}$，它的每个行向量记为对应样本$x_i$的$d’$维编码。</p><p>​        当现有的数据降维模型（编码模型）无法处理或者要付出很大资源的情况下，本文提出了一个深度神经网络，通过拟合谱分解，使该模型能够从节点的特征x，即可得到节点的编码。</p><ol><li><p>动机</p><p>2.1 谱分解问题涉及svd算法求解，该算法耗时很多$\mathcal O (n^3)$, 则新增out of sample节点，重启一次模型的代价很高。</p><p>2.2 对比的baseline算法，为$Nystr\ddot{o}m$拓展方法。对新增节点$x\in R^d$，第p维的拓展为</p><script type="math/tex; mode=display">y_p(x) = \frac{1}{\lambda_p} \sum_{i=1}^n v_{pi} K(x, x_i)</script><p>可以看出，该方法计算量会随着训练样本个数的增多，时间复杂度线性增加。</p></li><li><p>框架</p></li></ol><p>本文的思路很简洁，即训练一套DNN（记作y）去逼近映射上述谱分解映射$z: X\rightarrow U_{d’}$，记$z_i=z(x_i)$表示$U_{d’}$的第 i 行向量。</p><p>   3.1 DNN（encoder + decoder）</p><script type="math/tex; mode=display">h_l = \sigma(W_l h_{l-1} + b_l), \quad l=1,2,\dots,N</script><p>其中</p><script type="math/tex; mode=display">h_0 = x_i, \quad W_1\in R^{M\times d}, \quad W_l \in R^{M\times M} for l=2,3,\dots,N</script><p>于是</p><script type="math/tex; mode=display">y(x) = W_{N+1} h_N + b_{N+1}, \quad 其中\quad W_{N+1}\in R^{d'\times M},b_{N+1}\in R^{d'}.</script><p>训练的目标函数：</p><script type="math/tex; mode=display">\Theta^* = arg \min_{\Theta} \frac{1}{n}\sum_{i=1}^n ||z_i-y(x_i)||^2</script><p>作者在训练网络的时候，为了提高训练效率和结果的精度， 使用的初始化参数技巧如下：</p><p>3.1.1 无监督预训练</p><p>该部分是为了逐层初始化encoder的参数，即$W_l,b_i\quad for\quad l=1,2,\dots, N$. 每次引入一个隐层，加上一个临时的解码层$(W_{tmp}, b_{tmp})$,  优化目标:</p><script type="math/tex; mode=display">arg\min_{W_1,b_1,W_{tmp},b_{tmp}} \frac{1}{n}\sum_{i=1}^n ||x_i - y_{tmp}(x_i)||^2</script><p>其中</p><script type="math/tex; mode=display">y_{tmp}(x_i) = W_{tmp} h_1 + b_{tmp}, \quad h_1 = \sigma(W_1 h_0 + b_1)</script><p>经过几轮随机梯度下降优化所有参数，即通过拟合恒等映射，初始化$W_1,b_1$. 接下来，以$h_1$为输入，引入第二个隐层，再通过拟合恒等隐射，更新第二层隐层参数，以此类推，将N层隐层参数初始化。</p><p>3.1.2 有监督的修正(fine-tune)</p><p>使用预训练的参数为初始值，优化问题$\Theta^*$. 当DNN训练完成，对out of sample $x$, 即可计算其编码$y(x)$，且该计算消耗的时间与训练样本量无关。</p><ol><li><p>数值结果</p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200414122510170.png" alt="image-20200414122510170"></p><p>该表格的第一列，是训练数据中的节点个数，使用baseline和DNN方法得到新增节点的编码，使用NRMSE进行衡量，值越小越精准；时间的单位是秒，用来测试的新增节点个数为40w，可以看到baseline的计算时间是线性的，本文方法计算时间是长度。</p></li><li><p>备注</p></li></ol><p>工程实践中，如果已有了一套取得不错效果的图编码结果，为了处理新增节点的编码，需要一套与编码方式解耦的模型，能够利用已有的编码结果处理新增节点，同时期望这个新的编码能包含在已有的编码空间。</p><p>假设已有的编码函数为$f:X\rightarrow R^{d’}$, 本文通过训练一个DNN网络$y:X\rightarrow R^{d’}$，去逼近f，如此当新的节点x到来，该DNN可以给新节点一个编码$y(x)$， 且可以保证与f对应的空间相同。</p><p>假设使用node2vec已经得到一个图$G(V, E)$中节点的编码表embedding，复用训练的语料信息context,其中context的第i行为$c_i = [i, context_{i,1}, \cdots,context_{i,2n}]$, 其中2n为上下文的长度，再训练一个DNN网络$y: R^{2nd}\rightarrow R^d$， 使得$\min \sum_i||y(embedding[context_{i,1}, \cdots, context_{i,2n}])-embedding[i]||^2$ .</p>]]></content>
    
    
    <categories>
      
      <category>Graph Embedding</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding</title>
    <link href="/2021/09/16/LAN%20embedding/"/>
    <url>/2021/09/16/LAN%20embedding/</url>
    
    <content type="html"><![CDATA[<h3 id="Logic-Attention-Based-Neighborhood-Aggregation-for-Inductive-Knowledge-Graph-Embedding"><a href="#Logic-Attention-Based-Neighborhood-Aggregation-for-Inductive-Knowledge-Graph-Embedding" class="headerlink" title="Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding"></a>Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding</h3><ol><li><p>背景</p><p>基于知识图谱$G=(V, E)$, 如何进行知识的推理是十分重要的任务。假设$(s, o, r)$表示图谱中的一条边（发生的事件），其中s,o代表节点，r是关系类型，那么知识推理的一个例子就是：在已知s和r的情况下，推理出o是哪个节点。同理，在缺失s的情况下，预测s是哪个节点。本文没有考虑时间的因素，如果考虑$(s,o,r,t)$, 其中t是代表该事件发生的时间，更强大的知识图谱推理，具有补全实体或者时间的能力。</p><p>实现推理的一类工具是节点的编码。过去对于静态图的编码取得了一定的效果，如deep_walk, node2vec, Line等，但是这些编码方式，要求所有的节点都出现在图谱中；而现实中，随着时间推移，一定会有之前没包含在图谱的节点出现。面对这种情况，过去的静态编码方法，只能纳入新的数据，以全量数据重新训练模型。这在工程应用中，不仅计算代价可能较大，并且频繁的更新模型，会造成模型结果的不稳定。</p><p>本文的编码方法：具有推理能力，且可以处理新增节点的快速编码(out of sample).</p></li><li><p>动机</p><p>（1）当图编码过程完成后，快速的给新出现的节点进行编码。在训练编码的过程中，采用aggregator的方式，利用相邻节点以及涉及到的关系类型，对节点进行编码。该框架没有downstream任务，可以无监督的训练。</p><p>（2）本文主要是讲述构造aggregator的方式，其中作者想要构造的聚合器有如下性质:</p><p>​    · 聚合时，邻居节点是不应该考虑顺序的。例如简单的mean-pooling是可以的，而LSTM是不合适的，这一点应该根据实际问题出发，酌情考虑。作者的出发点是，当新增节点(emerging entity)出现时，节点Chicago_Bulls 和 American的重要性跟他们在邻居序列中的位置无关。</p><p>​    · 整体上来说，各类关系之间存在信息的包含，于是在聚合时，aggregator要有识别冗余信息(play_for 包含work_as的信息)以及关注重点关系（通常我们需要索引 live_in的时候，需要关注到与其相关的信息 e.g.play_for Chicago_Bulls）的机制。</p><p>​    <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200402143903095.png" alt="image-20200402143903095" style="zoom:67%;" /></p></li></ol><ol><li><p>框架</p><p>2.1 主要符号</p><p>$\varepsilon$ 和 $\mathcal{R}$表示节点和边的集合，数量分别为n和m. 知识图谱由一系列的三元组组成：</p><script type="math/tex; mode=display">\mathcal{K} = \{(s,r,o)|s\in \varepsilon , r\in \mathcal{R},o\in\varepsilon \}</script><p>对任意$\mathcal{K}$中的三元组(s,r,o),对应的加入(o, r^{-1}, o).</p><p>对任意的实体$e\in \varepsilon$，定义邻居（边和点的集合），及邻居在节点和关系集合的投影分别为:</p><script type="math/tex; mode=display">N_{\mathcal K}(e) = \{(r, e')|(e, r, e')\in \mathcal K  \}</script><script type="math/tex; mode=display">N_{\varepsilon}:N_{\mathcal K}(e) \rightarrow \varepsilon</script><script type="math/tex; mode=display">N_{\mathcal R} :N_{\mathcal K}(e) \rightarrow \mathcal R</script><p>2.2 目标</p><p>在给定的知识图谱上，学习一个邻居聚合器A，对一个节点$e_i \in \varepsilon$</p><script type="math/tex; mode=display">A: N_{\mathcal K}(e_i)\rightarrow R^d</script><p>并且对一个未知的三元组(s,r,o)的可能性建立在A的结果上, 为了区分邻居中的关系r，位置三元组改为(s, q, o)</p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200402152134913.png" alt="image-20200402152134913" style="zoom:50%;" /></p><p>2.3 encoder</p><p>如图二所示，对一个节点$e_i$（图中的乔丹大帝和芝加哥），实质是存在两套编码，即图中input embedding 和output embedding, encoder作用在$e_i$的邻居节点的输入编码上。令$e_i$的邻居$(r, e_j) \in N_{\mathcal K} (e_i)$, $e_j$的输入编码记为${e}_j^{I}$, 则特点关系的转移方程如下：</p><script type="math/tex; mode=display">T_r(\bold e_j^I) = \bold{e}_j^I - \bold{w}_r^T \bold{e}_j^I\bold{w}_r,</script><p>目标$e_i$输出编码为：</p><script type="math/tex; mode=display">\bold{e}_i^O = A(\{(r,e_j)\in N_{\mathcal K}(i) \}).</script><p>A的作用实质上是将一堆d维向量，聚集成一根d维向量。作为对比，可以选择简单的平均，以及复杂的LSTM.</p><p>本文采取加权求和的方式，聚合向量:</p><script type="math/tex; mode=display">\bold{e}_i^O = \sum_{(r,e_j)\in N_{\mathcal K}(i)}\alpha_{j|i,q} T_r(\bold{e}_j^I)</script><p>为了使A满足上述的性质， 需仔细的构造$\alpha_{j|i,q}$，该系数不仅与邻居节点编码有关，也与邻居关系r和需要索引的关系q有关。</p><p>2.3.1 逻辑法则机制(logic rule mechanism)</p><p>对一个节点e， 它的邻居关系$r_1, r_2$，可能存在信息的包含关系 ，比如play_for可能推导出live_in，记他们的潜在关联规则为$r_1 \Rightarrow r_2$，定义规则的置信度为：</p><script type="math/tex; mode=display">\mathcal P(r_1\Rightarrow r_2)= \frac{\sum_{e\in \varepsilon}1(r_1\in N_{\mathcal R}(e) \wedge r_2\in N_{\mathcal R}(e)) }{ \sum_{e\in \varepsilon}1(r_1\in N_{\mathcal R}(e))}.</script><script type="math/tex; mode=display">\alpha_{j|i,q}^{Logic} = \frac{\mathcal P(r\Rightarrow q)}{max(\{\mathcal P(r'\Rightarrow r)|r' \in N_{\mathcal R}(e_i) \wedge r'\neq r\})}</script></li></ol><p>   2.3.2 神经网络机制(neural network mechanism)</p><p>   给定关系q，$e_i$的邻居$e_j$的重要性为：</p><script type="math/tex; mode=display">   \alpha_{j|i,q}^{NN} = softmax(\alpha_{j|i,q}')=\frac{exp(\alpha'_{j|i,q})}{\sum_{j'\in N_{\varepsilon}(i)} exp(\alpha'_{j'|i,q})}</script><p>   其中：</p><script type="math/tex; mode=display">   \alpha_{j|i,q}'=\bold u_a^T \cdot tanh(\bold W_a \cdot [z_q ; T_r({\bold e}_j^I)])</script><p>   ${\bold u}_a^T$, ${\bold W}_a$ 和$\bold z_q$是训练参数。</p><p>   综合2.3.1和2.3.2，：</p><script type="math/tex; mode=display">   \bold e_i^O = \sum_{(r,e_j)\in N_{\mathcal K}(i)} (\alpha_{j|i,q}^{Logic} + \alpha_{j|i,q}^{NN}) T_r(\bold e_j^I)</script><p>   2.4 decoder</p><p>   从encoder， 给出施加方和承受方的编码${\bold s}^O$和${\bold o}^O$, decoder被用来衡量该训练三元组的合理性(plausibility)，$\bold W_r  \in R^{m\times d}$是用来做关系转移的那个矩阵见2.3第一个方程。则decoder给(s,q,o)的打分函数为:</p><script type="math/tex; mode=display">   \phi^O(s,q,o)=-|\bold s^O + \bold q - \bold o^O|_{L1},</script><p>   2.5 目标函数</p><p>   为了训练模型，需要构造正样本和负样本，$\mathcal K$中的三元组都是正样本，记作$\Delta$, 对一个正样本(s,q,o)，随机替换s或者o，不同时替换：</p><script type="math/tex; mode=display">   \Delta'_{(s,q,o)} = \{(s',q,o)|s'\in \varepsilon\} \cup \{(s,q,o')|o'\in \varepsilon\}</script><script type="math/tex; mode=display">   l^O(s,q,o) = [\gamma - \phi^O(s,q,o)+\phi^O(s',q,o')]_+</script><p>   其中$[x]_+ = max(0, x)$, $\gamma$是一个超参数。于是训练的目标函数为：</p><script type="math/tex; mode=display">   min \sum_{(s,q,o)\in\Delta}\sum_{(s',q,o')\in \Delta'_{(s,q,o)}} l^O(s,q,o).</script><ol><li><p>备注</p><p>（1）本文在编码器中整合aggregator，解决了网络新增节点的问题，预计速度较快，因为只用lookup几个邻居节点和边的编码，加权求和即可；</p><p>（2）本文是通过糅合聚合器，同时解决编码和新增节点问题，如果工程上已有取得不错效果的编码（如node2vec，line等），可能需要一个独立于编码器的组件，来处理新增节点。可见本专栏另一篇关于恶意软件的论文分享。</p><p>（3）聚合器中的权重，既可以从神经网络中自动提取，同时也可以人为定义；作者提出的聚合器需要满足的性质，可根据应用的具体场景而定，比如邻居关系与发生的时间有紧密关系，则可加入按时间衰减的机制，或者邻居根据发生的时间有天然的序列属性，使用LSTM有可能会挖掘到高度非线性特征，取得更好的效果。</p><p>（4）极端的情况下，新增的节点未与任何已有节点相连，模型就会失效。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Graph Embedding</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Linux chattr 命令详解</title>
    <link href="/2021/08/06/linux_command_chattr/"/>
    <url>/2021/08/06/linux_command_chattr/</url>
    
    <content type="html"><![CDATA[<h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><blockquote><p>chattr命令用于更改Linux文件的属性，比如设置文件为不可删除、只读等类型。只有具有sudo权限的用户才能执行这项命令。chattr可看成是change attributes的缩写：ch+attr</p><p>lsattr可查看chattr命令执行后的文件属性</p></blockquote><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">chattr [ -RVf ] [ -v version ] [ mode ] files<br></code></pre></td></tr></table></figure><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs bash">可选参数：[ -RVf ] 和 [-v version]<br>-R: 递归处理，将指定目录下的所有文件及子目录一并处理.<br>-V: 显示输出指令执行过程<br>-f: 若该文件权限无法被更改也不要显示错误讯息<br>-v version: 设置文件或目录版本<br><br><br>必选参数：[ mode ]<br>+&lt;属性&gt;：开启文件的该项属性<br>-&lt;属性&gt;：关闭文件的该项属性<br>=&lt;属性&gt;：使得文件只有该项属性<br><br><br>属性：<br>a: 针对文件：只允许程序在文件后面追加数据。针对文件夹：只允许创建和修改文件内容，但不允许删除<br>A: 告诉系统不修改该文件的最后访问时间，以减小磁盘I/O负载<br>b: 不更新文件的最后存取时间<br>c: 将文件或目录压缩后存放<br>C: 不执行写入时复制，多个用户对同一资源进行写操作时，不生成副本<br>d: 当dump程序执行时，该文件或目录不会被dump备份<br>D: 同步更新文件或目录<br>e: 使用extent文件格式<br>i: 不得任意改动文件或目录<br>j: 数据在写入到文件前暂存到journal<br>s: 保密性删除文件或目录<br>S: 及时更新文件或目录<br>t: 不支持文件尾部合并<br>T: 使得目录下的文件存放位置不存在联系，因为一般而言，同一目录下的文件在硬盘上的存放位置越相邻越好<br>u: 预防意外删除<br><br></code></pre></td></tr></table></figure><h3 id="示例大全"><a href="#示例大全" class="headerlink" title="示例大全"></a>示例大全</h3><h4 id="设置文件为只读、不可删除"><a href="#设置文件为只读、不可删除" class="headerlink" title="设置文件为只读、不可删除"></a>设置文件为只读、不可删除</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># test.txt文件</span><br>sudo chattr +i test.txt<br></code></pre></td></tr></table></figure><h4 id="设置目录下的所有文件为只读、不可删除"><a href="#设置目录下的所有文件为只读、不可删除" class="headerlink" title="设置目录下的所有文件为只读、不可删除"></a>设置目录下的所有文件为只读、不可删除</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># home目录</span><br>sudo chattr -R +i /home/<br></code></pre></td></tr></table></figure><h4 id="设置文件为只能追加、不可删除"><a href="#设置文件为只能追加、不可删除" class="headerlink" title="设置文件为只能追加、不可删除"></a>设置文件为只能追加、不可删除</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo chattr +a test.txt<br></code></pre></td></tr></table></figure><h4 id="设置目录下的所有文件只能追加、不可删除"><a href="#设置目录下的所有文件只能追加、不可删除" class="headerlink" title="设置目录下的所有文件只能追加、不可删除"></a>设置目录下的所有文件只能追加、不可删除</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo chattr -R +a /home/<br></code></pre></td></tr></table></figure><h4 id="取消文件的只读、不可删除"><a href="#取消文件的只读、不可删除" class="headerlink" title="取消文件的只读、不可删除"></a>取消文件的只读、不可删除</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># test.txt文件</span><br>sudo chattr -i test.txt<br></code></pre></td></tr></table></figure><h4 id="取消目录下的所有文件只读、不可删除"><a href="#取消目录下的所有文件只读、不可删除" class="headerlink" title="取消目录下的所有文件只读、不可删除"></a>取消目录下的所有文件只读、不可删除</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># home目录</span><br>sudo chattr -R -i /home/<br></code></pre></td></tr></table></figure><h4 id="取消文件的只能追加、不可删除"><a href="#取消文件的只能追加、不可删除" class="headerlink" title="取消文件的只能追加、不可删除"></a>取消文件的只能追加、不可删除</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo chattr -a test.txt<br></code></pre></td></tr></table></figure><h4 id="取消目录下的所有文件只能追加、不可删除"><a href="#取消目录下的所有文件只能追加、不可删除" class="headerlink" title="取消目录下的所有文件只能追加、不可删除"></a>取消目录下的所有文件只能追加、不可删除</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo chattr -R -a /home/<br></code></pre></td></tr></table></figure><h4 id="查看文件属性"><a href="#查看文件属性" class="headerlink" title="查看文件属性"></a>查看文件属性</h4><blockquote><p>使用lsattr命令</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看test.txt文件的属性</span><br>lsattr test.txt<br><br><span class="hljs-comment"># 查看当前目录下所有文件的属性</span><br>lsattr ./<br><br><span class="hljs-comment"># 查看所有以wav后缀结尾的文件的属性</span><br>lsattr *.wav<br></code></pre></td></tr></table></figure><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><blockquote><p>chattr 并不适用于所有目录，chattr不能保护/, /dev, /var目录</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Linux 常用命令详解</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/08/04/hello-world/"/>
    <url>/2021/08/04/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    <categories>
      
      <category>Start Hexo</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
